# Consumer properties
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
max.poll.interval.ms=300000
enable.auto.commit=false
auto.offset.reset=earliest
max.poll.records=1000
fetch.min.bytes=100000

# Producer properties
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer

# Confluent Parallel Consumer properties
parallel.consumer.max.concurrency=20
parallel.consumer.order=UNORDERED
parallel.consumer.commit.mode=PERIODIC_CONSUMER_ASYNCHRONOUS
# parallel.consumer.seconds.between.commits=60

# Application-specific properties
input.topic.name=parallel-consumer-input-topic
output.topic.name=parallel-consumer-output-topic
records.to.consume=100000

# Confluent Cloud
bootstrap.servers=
security.protocol=SASL_SSL
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='' password='';
sasl.mechanism=PLAIN
acks=all
schema.registry.url=
basic.auth.credentials.source=USER_INFO
basic.auth.user.info=
