# Consumer properties
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
max.poll.interval.ms=300000
enable.auto.commit=false
auto.offset.reset=earliest
max.poll.records=1000
fetch.min.bytes=100000

# Producer properties
key.serializer=org.apache.kafka.common.serialization.StringSerializer
value.serializer=org.apache.kafka.common.serialization.StringSerializer

# Confluent Parallel Consumer properties
parallel.consumer.max.concurrency=20
parallel.consumer.order=UNORDERED
parallel.consumer.commit.mode=PERIODIC_CONSUMER_ASYNCHRONOUS
# parallel.consumer.seconds.between.commits=60

# Application-specific properties
input.topic.name=parallel-consumer-input-topic
output.topic.name=parallel-consumer-output-topic
records.to.consume=100000

# Confluent Cloud
bootstrap.servers=pkc-w12qj.ap-southeast-1.aws.confluent.cloud:9092
security.protocol=SASL_SSL
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='T4DGY7GQ6IUUSCI6' password='EhhdysCbY1mpuK6L7Hhv7PLL3iUkER+1M/oM4AbBFmkRruP0HIvmwmjJfUrSJG+c';
sasl.mechanism=PLAIN
acks=all
schema.registry.url=https://psrc-7q7vj.ap-southeast-2.aws.confluent.cloud
basic.auth.credentials.source=USER_INFO
basic.auth.user.info=PKPO7WW723KJJS5F:fHJfnGRwdpGnJ0bZ/FsmTtnGgj1s++2KQx+DOaSDDOVAZlnCcnPeJseWQQRJJpVb
